{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ViT Assignment\n",
        "Authors: Alexander Wan, Aryan Jain"
      ],
      "metadata": {
        "id": "Tt6WRWv2JXJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Goals\n",
        "\n",
        "\n",
        "1. Familiarity with the Vision Transformer architecture\n",
        "2. Familiarity with the self-attention algorithm\n",
        "3. Practice with PyTorch matrix operations\n",
        "\n"
      ],
      "metadata": {
        "id": "uTUA3I0hT2EM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "1. Implement multi-head self-attention\n",
        "2. Incorporate that into a ViT"
      ],
      "metadata": {
        "id": "QM620ckwWDBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime Acceleration\n",
        "Colab limits GPU usage, so set `device` below as `'cpu'` and change your runtime to CPU as well (Runtime > Change runtime type) when you're developing, and only change it to `'cuda'` (and your runtime to GPU) when you're ready to train."
      ],
      "metadata": {
        "id": "_U_6XOlXM61e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device = 'cpu'\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "dbLiRflPIoPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head self-attention\n",
        "Begin by implementing multiheaded self-attention. Do **not** use any `for` loops, and instead put all of the calculations into [batch matrix multiplications](https://pytorch.org/docs/stable/generated/torch.bmm.html) or [Linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
        "\n",
        "Useful references include the lecture slides and the [illustrated transformer](https://jalammar.github.io/illustrated-transformer/).\n"
      ],
      "metadata": {
        "id": "VCsSGbgWWul-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class MSA(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, num_heads):\n",
        "      super().__init__()\n",
        "      self.n_heads = num_heads\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dim = input_dim\n",
        "      self.head_dim = self.embed_dim // self.n_heads\n",
        "\n",
        "      # Linear projections for Q, K, and V\n",
        "\n",
        "      self.wq = nn.Linear(self.dim, self.embed_dim, bias=False)\n",
        "      self.wk = nn.Linear(self.dim, self.embed_dim, bias=False)\n",
        "      self.wv = nn.Linear(self.dim, self.embed_dim, bias=False)\n",
        "      self.wo = nn.Linear(self.embed_dim, self.dim, bias=False)\n",
        "\n",
        "      #torch.Size([512, 66, 384])\n",
        "      #torch.Size([512, 66, 384])\n",
        "      #torch.Size([512, 66, 6, 64])\n",
        "      #torch.Size([512, 6, 66, 64])\n",
        "      #torch.Size([512, 6, 66, 66])\n",
        "      #torch.Size([512, 6, 66, 64])\n",
        "      #torch.Size([512, 66, 384])\n",
        "\n",
        "  def forward(self, x):\n",
        "      b, seq_len, dim = x.shape  # b: batch size, seq_len: sequence length\n",
        "\n",
        "      assert dim == self.dim, \"dim is not matching\"\n",
        "\n",
        "      q = self.wq(x)  # [b, seq_len, n_heads*head_dim]\n",
        "      k = self.wk(x)  # [b, seq_len, n_heads*head_dim]\n",
        "      v = self.wv(x)  # [b, seq_len, n_heads*head_dim]\n",
        "\n",
        "\n",
        "        # Reshape the tensors for multi-head operations\n",
        "      q = q.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "      k = k.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "      v = v.contiguous().view(b, seq_len, self.n_heads, self.head_dim)  # [b, seq_len, n_heads, head_dim]\n",
        "\n",
        "\n",
        "        # Transpose to bring the head dimension to the front\n",
        "      q = q.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "      k = k.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "      v = v.transpose(1, 2)  # [b, n_heads, seq_len, head_dim]\n",
        "\n",
        "\n",
        "        # Compute attention scores and apply softmax\n",
        "      attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)  # [b, n_heads, seq_len, seq_len]\n",
        "\n",
        "      attn_scores = F.softmax(attn, dim=-1)  # [b, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # Compute the attended features\n",
        "      out = torch.matmul(attn_scores, v)\n",
        "\n",
        "        # [b, n_heads, seq_len, head_dim]\n",
        "      out = out.contiguous().view(b, seq_len, -1)  # [b, seq_len, n_heads*head_dim]\n",
        "\n",
        "      return self.wo(out)  # [b, seq_len, dim]"
      ],
      "metadata": {
        "id": "_q-UREAdT1WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the ViT architecture\n",
        "You will be implementing the ViT architecture based on the \"An image is worth 16x16 words\" paper.\n",
        "\n",
        "Although the ViT and Transformer architecture are very similar, note a few differences:\n",
        "\n",
        "1. Image patches instead of discrete tokens as input.\n",
        "2. [GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) is used for the linear layers in the transformer layer (instead of ReLU)\n",
        "3. LayerNorm before the sublayer instead of after.\n",
        "4. Dropout after every linear layer except for KQV projections and also directly after adding positional embeddings to the patch embeddings.\n",
        "5. Learnable [CLS] token at the beginning of the input.\n",
        "\n",
        "A useful reference is Figure 1 in the [paper](https://arxiv.org/pdf/2010.11929.pdf)."
      ],
      "metadata": {
        "id": "ZoajGRYKsI29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, implement a single layer:"
      ],
      "metadata": {
        "id": "Blpaw39U5Y9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTLayer(nn.Module):\n",
        "  def __init__(self, num_heads, input_dim, embed_dim, mlp_hidden_dim, dropout=0.1):\n",
        "    '''\n",
        "    num_heads: Number of heads for multi-head self-attention\n",
        "    embed_dim: Dimension of internal key, query, and value embeddings\n",
        "    mlp_hidden_dim: Hidden dimension of the linear layer\n",
        "    dropout: Dropout rate\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.msa = MSA(input_dim, embed_dim, num_heads)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "    self.w_o_dropout = nn.Dropout(dropout)\n",
        "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "                              nn.GELU(),\n",
        "                              nn.Dropout(dropout),\n",
        "                              nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "                              nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: input embeddings (batch_size, max_length, input_dim)\n",
        "    return: output embeddings (batch_size, max_length, embed_dim)\n",
        "    '''\n",
        "    res = x\n",
        "    x += self.w_o_dropout(self.msa(self.layernorm1(x)))\n",
        "    x += self.mlp(self.layernorm2(x))\n",
        "    return x\n",
        "    # TODO: Fill in the code for the forward pass below\n",
        "    # You shouldn't need to initialize any more modules, everything you need is already\n",
        "    # in __init__\n",
        "    # A forward function consists of:\n",
        "    # 1) LayerNorm of x\n",
        "    # 2) Self-Attention on output of 1)\n",
        "    # 3) Dropout\n",
        "    # 4) Residual w/ original x\n",
        "    # 5) LayerNorm\n",
        "    # 6) MLP\n",
        "    # 7) Residual\n"
      ],
      "metadata": {
        "id": "DffgJiJDsA9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A portion of the full network is already implemented for you. Your task is to implement the preprocessing code, converting raw images into patch embeddings + positional embeddings + dropout, with a learnable CLS token at the beginning of the input.\n",
        "\n",
        "Note that patch embeddings are to be added to positional embeddings elementwise, so the input embedding dimensions is size embed_dim."
      ],
      "metadata": {
        "id": "o8CONmQMfK4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, patch_dim, image_dim, num_layers, num_heads, embed_dim, mlp_hidden_dim, num_classes, dropout):\n",
        "    '''\n",
        "    patch_dim: patch length and width to split image by\n",
        "    image_dim: image length and width\n",
        "    num_layers: number of layers in network\n",
        "    num_heads: number of heads for multi-head attention\n",
        "    embed_dim: dimension to project images patches to and dimension to use for position embeddings\n",
        "    mlp_hidden_dim: hidden dimension of linear layer\n",
        "    num_classes: number of classes to classify in data\n",
        "    dropout: dropout rate\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.patch_dim = patch_dim\n",
        "    self.image_dim = image_dim\n",
        "    self.input_dim = self.patch_dim * self.patch_dim * 3\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.patch_embedding = nn.Linear(self.input_dim, embed_dim)\n",
        "    self.position_embedding = nn.Parameter(torch.zeros(1, (image_dim // patch_dim) ** 2 + 1, embed_dim))\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([])\n",
        "    for i in range(num_layers):\n",
        "      self.encoder_layers.append(ViTLayer(num_heads, embed_dim, embed_dim, mlp_hidden_dim, dropout))\n",
        "\n",
        "    self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "    self.layernorm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, images):\n",
        "    '''\n",
        "    images: raw image data (batch_size, channels, rows, cols)\n",
        "    '''\n",
        "\n",
        "    # Don't hardcode dimensions (except for maybe channels = 3), use the variables in __init__.\n",
        "    # You shouldn't need to add anything else to __init__, all of the embeddings,\n",
        "    # dropout etc. are already initialized for you.\n",
        "\n",
        "    # Put the preprocessed patches in variable \"out\" with shape (batch_size, length, embed_dim).\n",
        "\n",
        "    # HINT: You can make image patches with .reshape\n",
        "    # e.g.\n",
        "    # x = torch.ones((100, 100))\n",
        "    # x_patches = x.reshape(4, 25, 4, 25)\n",
        "    # where you have 4 * 4 patches with each patch being 25 by 25\n",
        "    w = self.image_dim // self.patch_dim\n",
        "    h = w\n",
        "    N = images.size(0)\n",
        "    images = images.reshape(N, 3, h, self.patch_dim, w, self.patch_dim)\n",
        "    images = torch.einsum(\"nchpwq -> nhwpqc\", images)\n",
        "    patches = images.reshape(N, h * w, self.input_dim) # (batch, num_patches_per_image, patch_size_unrolled)\n",
        "\n",
        "    patch_embeddings = self.patch_embedding(patches)# TODO: Pass through our patch embedding layer\n",
        "    patch_embeddings = torch.cat([torch.tile(self.cls_token, (N, 1, 1)),\n",
        "                                  patch_embeddings], dim=1)\n",
        "    out = patch_embeddings + torch.tile(self.position_embedding, (N, 1, 1)) # We add positional embeddings to our tokens (not concatenated)\n",
        "    out = self.embedding_dropout(out) # TODO: Pass through our embedding dropout layer\n",
        "\n",
        "    # add padding s.t. input length is multiple of num_heads\n",
        "    add_len = (self.num_heads - out.shape[1]) % self.num_heads\n",
        "    out = torch.cat([out, torch.zeros(N, add_len, out.shape[2], device=device)], dim=1)\n",
        "\n",
        "    # TODO: Pass through each one of our encoder layers\n",
        "    for layer in self.encoder_layers:\n",
        "      #print(layer)\n",
        "      out = layer(out)\n",
        "    # Pop off and read our classification token we added, see what the value is\n",
        "    cls_head = self.layernorm(torch.squeeze(out[:, 0], dim=1))\n",
        "    logits = self.mlp_head(cls_head)\n",
        "    return logits\n",
        "\n",
        "def get_vit_tiny(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=3,\n",
        "              embed_dim=192, mlp_hidden_dim=768, num_classes=num_classes, dropout=0.1)\n",
        "\n",
        "def get_vit_small(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=6,\n",
        "               embed_dim=384, mlp_hidden_dim=1536, num_classes=num_classes, dropout=0.1)"
      ],
      "metadata": {
        "id": "TKbFJj447myz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train the model! You don't need to write any code for this - just run the cell.\n",
        "\n",
        "Remember to change the device variable (in the cell at the beginning of the notebook) to 'cuda' and change your runtime to GPU (Runtime > Change runtime type) as well.\n",
        "\n",
        "Try to get 60%+ accuracy after 30 epochs."
      ],
      "metadata": {
        "id": "ZJwzQU0qMt9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "cifar10_mean = torch.tensor([0.49139968, 0.48215827, 0.44653124])\n",
        "cifar10_std = torch.tensor([0.24703233, 0.24348505, 0.26158768])\n",
        "\n",
        "class Cifar10Dataset(Dataset):\n",
        "    def __init__(self, train):\n",
        "        self.transform = transforms.Compose([\n",
        "                                                transforms.Resize(40),\n",
        "                                                transforms.RandomCrop(32),\n",
        "                                                transforms.RandomHorizontalFlip(),\n",
        "                                                transforms.ToTensor(),\n",
        "                                                transforms.Normalize(cifar10_mean, cifar10_std)\n",
        "                                            ])\n",
        "        self.dataset = torchvision.datasets.CIFAR10(root='./SSL-Vision/data',\n",
        "                                                    train=train,\n",
        "                                                    download=True)\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "trainset = Cifar10Dataset(True)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)#create dataloader\n",
        "# TODO: Pass our dataset trainset into a torch Dataloader object, with shuffle = True and the batch_size=batch_size, num_workers=2\n",
        "\n",
        "testset = Cifar10Dataset(False)\n",
        "testloader = DataLoader(trainset, batch_size=1, shuffle=False, num_workers=2)# TODO: create a test dataset the same as the train loader but with shuffle=False and the test dataset\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n"
      ],
      "metadata": {
        "id": "fkN5vhqp9eI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = get_vit_small().to(device)\n",
        "vit = torch.nn.DataParallel(vit)\n",
        "\n",
        "learning_rate = 5e-4 * batch_size / 256\n",
        "num_epochs = 30\n",
        "warmup_fraction = 0.1\n",
        "weight_decay = 0.1\n",
        "\n",
        "total_steps = math.ceil(len(trainset) / batch_size) * num_epochs\n",
        "# total_steps = num_epochs\n",
        "warmup_steps = total_steps * warmup_fraction\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(vit.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=weight_decay)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_total = 0\n",
        "    vit.train()\n",
        "    for inputs, labels in tqdm(trainloader):\n",
        "        \"\"\"TODO:\n",
        "        1. Set inputs and labels to be on device\n",
        "        2. zero out our gradients\n",
        "        3. pass our inputs through the ViT\n",
        "        4. pass our outputs / labels into our loss / criterion\n",
        "        5. backpropagate\n",
        "        6. step our optimizeer\n",
        "        \"\"\"\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = vit(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "        train_acc += torch.sum((torch.argmax(output, dim=1) == labels)).item()\n",
        "        train_total += inputs.shape[0]\n",
        "    train_loss = train_loss / train_total\n",
        "    train_acc = train_acc / train_total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    test_total = 0\n",
        "    vit.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "\n",
        "            test_loss += loss.item() * inputs.shape[0]\n",
        "            test_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n",
        "            test_total += inputs.shape[0]\n",
        "    test_loss = test_loss / test_total\n",
        "    test_acc = test_acc / test_total\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'[{epoch + 1:2d}] train loss: {train_loss:.3f} | train accuracy: {train_acc:.3f} | test_loss: {test_loss:.3f} | test_accuracy: {test_acc:.3f}')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "5-YUaj0-Xvpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vit.module.state_dict(), \"ViT_MSA_BerkleyHW4.pth\")"
      ],
      "metadata": {
        "id": "DOGnhKAFHvQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RpgfDyOrrF5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6MTW9JJjOczM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vit.eval()\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in testloader:\n",
        "    inputs = inputs.to('cuda:0')\n",
        "    output = vit(inputs)\n",
        "    _, pred = torch.max(output, dim=1)\n",
        "    print(f\"ViT CLS token Classification: {pred.detach().item()}\")\n",
        "    print(f\"Ground Truth label: {labels.item()}\")\n",
        "    sleep(0.5)"
      ],
      "metadata": {
        "id": "d5UhU7lWN3P3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}